{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc1b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint e:\\LPU\\Build-a-thon\\.venv\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: en (0.74) in first 30s of audio...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 138: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 3) Load your lyrics in exact order\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdata/shapeofyou.mp3\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     lyrics = \u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 4) Match each lyric line to the best window of words\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatch_lyrics_to_words\u001b[39m(lyrics, words):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xff in position 138: invalid start byte"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "from difflib import SequenceMatcher\n",
    "import datetime\n",
    "\n",
    "# 1) Load audio & models\n",
    "device     = \"cpu\"\n",
    "audio_path = \"data/shapeofyou.mp3\"\n",
    "model      = whisperx.load_model(\n",
    "                \"base\",\n",
    "                device=device,\n",
    "                compute_type=\"float32\"   # ← force float32 on CPU\n",
    "             )\n",
    "\n",
    "audio      = whisperx.load_audio(audio_path)\n",
    "\n",
    "# 2) Transcribe & get word timestamps\n",
    "result       = model.transcribe(audio_path)\n",
    "segments     = result[\"segments\"]\n",
    "align_model, metadata = whisperx.load_align_model(\n",
    "    language_code=result[\"language\"],\n",
    "    device=device\n",
    ")\n",
    "result_aligned = whisperx.align(\n",
    "    segments, align_model, metadata, audio, device\n",
    ")\n",
    "words        = result_aligned[\"word_segments\"]\n",
    "words     = result_aligned[\"word_segments\"]       # list of dicts: {word, start, end}\n",
    "\n",
    "# 3) Load your lyrics in exact order\n",
    "with open(\"data/shapeofyou.mp3\", encoding=\"utf-8\") as f:\n",
    "    lyrics = [L.strip() for L in f if L.strip()]\n",
    "\n",
    "# 4) Match each lyric line to the best window of words\n",
    "def match_lyrics_to_words(lyrics, words):\n",
    "    aligned = []\n",
    "    used_idxs = set()\n",
    "    for line in lyrics:\n",
    "        # find the contiguous run of words whose joined text best matches the lyric\n",
    "        best = {\"score\":0, \"start\":None, \"end\":None}\n",
    "        for i in range(len(words)):\n",
    "            if i in used_idxs: continue\n",
    "            text_accum = \"\"\n",
    "            for j in range(i, len(words)):\n",
    "                if j in used_idxs: break\n",
    "                text_accum += words[j][\"word\"] + \" \"\n",
    "                score = SequenceMatcher(None,\n",
    "                                        line.lower(),\n",
    "                                        text_accum.strip().lower()\n",
    "                                       ).ratio()\n",
    "                if score > best[\"score\"]:\n",
    "                    best = {\n",
    "                        \"score\": score,\n",
    "                        \"start\": words[i][\"start\"],\n",
    "                        \"end\":   words[j][\"end\"],\n",
    "                        \"indexes\": list(range(i, j+1))\n",
    "                    }\n",
    "        if best[\"start\"] is not None:\n",
    "            aligned.append({\n",
    "                \"lyric\": line,\n",
    "                \"start\": best[\"start\"],\n",
    "                \"end\":   best[\"end\"]\n",
    "            })\n",
    "            used_idxs.update(best[\"indexes\"])\n",
    "        else:\n",
    "            aligned.append({\"lyric\": line, \"start\": None, \"end\": None})\n",
    "    return aligned\n",
    "\n",
    "aligned = match_lyrics_to_words(lyrics, words)\n",
    "\n",
    "# 5) Enforce strictly sequential, non‑overlapping timing\n",
    "def enforce_sequence(aligned, min_gap=0.01):\n",
    "    last_end = 0.0\n",
    "    for e in aligned:\n",
    "        s = e[\"start\"] if e[\"start\"] is not None else last_end\n",
    "        t = e[\"end\"]   if e[\"end\"]   is not None else s + min_gap\n",
    "        if s < last_end:\n",
    "            s = last_end\n",
    "        if t <= s:\n",
    "            t = s + min_gap\n",
    "        e[\"start\"], e[\"end\"] = s, t\n",
    "        last_end = t\n",
    "    return aligned\n",
    "\n",
    "aligned = enforce_sequence(aligned)\n",
    "\n",
    "# 6) Write SRT\n",
    "def fmt(ts):\n",
    "    td = datetime.timedelta(seconds=ts)\n",
    "    tot = int(td.total_seconds())\n",
    "    hh = tot//3600; mm = (tot%3600)//60; ss = tot%60\n",
    "    ms = int((td.total_seconds()-tot)*1000)\n",
    "    return f\"{hh:02d}:{mm:02d}:{ss:02d},{ms:03d}\"\n",
    "\n",
    "with open(\"output1.srt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, e in enumerate(aligned, 1):\n",
    "        f.write(f\"{i}\\n{fmt(e['start'])} --> {fmt(e['end'])}\\n{e['lyric']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78597d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  14%|█▎        | 1180/8663 [01:49<08:41, 14.34it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video lyrics_video2.mp4.\n",
      "MoviePy - Writing audio in lyrics_video2TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  14%|█▎        | 1180/8663 [01:49<08:41, 14.34it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video lyrics_video2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  14%|█▎        | 1180/8663 [02:31<08:41, 14.34it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready lyrics_video2.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.video.VideoClip import ImageClip, TextClip\n",
    "from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pysrt\n",
    "\n",
    "# --- STEP 1: Load audio ---\n",
    "audio = AudioFileClip(\"data/shapeofyou.mp3\")\n",
    "audio_duration = audio.duration\n",
    "\n",
    "# --- STEP 2: Load and resize background image using Pillow ---\n",
    "img = Image.open(\"data/eagle.jpg\")\n",
    "img_resized = img.resize((int(720 * img.width / img.height), 720), Image.Resampling.LANCZOS)\n",
    "img_array = np.array(img_resized)\n",
    "background = ImageClip(img_array, duration=audio_duration)\n",
    "\n",
    "# --- STEP 3: Load SRT file ---\n",
    "subs = pysrt.open(\"output1.srt\")\n",
    "\n",
    "# --- STEP 4: Create text overlays for each subtitle line ---\n",
    "text_clips = []\n",
    "last_end = 0.0  # time in seconds when the last subtitle finished\n",
    "\n",
    "for sub in subs:\n",
    "    # get precise start/end in seconds (including milliseconds)\n",
    "    start = sub.start.ordinal / 1000.0\n",
    "    end   = sub.end.ordinal   / 1000.0\n",
    "\n",
    "    # if this subtitle would overlap the previous one, push its start forward\n",
    "    if start < last_end:\n",
    "        start = last_end\n",
    "\n",
    "    # recalc duration; skip if non‑positive\n",
    "    duration = end - start\n",
    "    if duration <= 0:\n",
    "        continue\n",
    "\n",
    "    txt = TextClip(\n",
    "        text=sub.text,\n",
    "        font_size=48,\n",
    "        font=r'C:\\WINDOWS\\FONTS\\COPRGTL.TTF',\n",
    "        color=\"white\",\n",
    "        bg_color=\"black\",\n",
    "        method='label',\n",
    "        transparent=False\n",
    "    )\n",
    "    txt = txt.with_start(start).with_duration(duration).with_position('center')\n",
    "\n",
    "    text_clips.append(txt)\n",
    "    last_end = start + duration\n",
    "\n",
    "# --- STEP 5: Combine background + text overlays ---\n",
    "final_video = CompositeVideoClip([background] + text_clips)\n",
    "final_video = final_video.with_audio(audio)\n",
    "\n",
    "# --- STEP 6: Export final video ---\n",
    "final_video.write_videofile(\"lyrics_video2.mp4\", fps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b92bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
